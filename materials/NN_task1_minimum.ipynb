{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler, RobustScaler, LabelEncoder\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TensorFlow:\n",
    "Взять существующий код для простой линейной регрессии (из блокнота Tensorflow examples 1) и переделать его в модель для решения задачи классификации. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Step 1: init dataset\n",
    "X_data, y_data = make_classification(n_samples=2000, n_classes=2, n_features=5,\n",
    "                                     n_informative=3, n_redundant=2, random_state=42)\n",
    "\n",
    "# One-hot encoding for labels\n",
    "y_data = np.array([y_data, -(y_data-1)]).T\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42) \n",
    "n_train_samples = X_train.shape[0]\n",
    "n_test_samples = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_clf_X_train, rand_clf_X_test, rand_clf_y_train, rand_clf_y_test = X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 20\n",
    "LEARNING_RATE = 0.01\n",
    "n_classes = 2\n",
    "n_features = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 1: 0.5199011374264956\n",
      "Average loss epoch 2: 0.33693688865751026\n",
      "Average loss epoch 3: 0.25311205461621283\n",
      "Average loss epoch 4: 0.204761229082942\n",
      "Average loss epoch 5: 0.17793664168566464\n",
      "Average loss epoch 6: 0.15619511669501662\n",
      "Average loss epoch 7: 0.1371015662327409\n",
      "Average loss epoch 8: 0.1364249404054135\n",
      "Average loss epoch 9: 0.11322052618488669\n",
      "Average loss epoch 10: 0.11729976823553442\n",
      "Average loss epoch 11: 0.10393875571899117\n",
      "Average loss epoch 12: 0.10380841335281729\n",
      "Average loss epoch 13: 0.08560977661982179\n",
      "Average loss epoch 14: 0.0909971239976585\n",
      "Average loss epoch 15: 0.08282982306554913\n",
      "Average loss epoch 16: 0.08140875990502536\n",
      "Average loss epoch 17: 0.07453312771394849\n",
      "Average loss epoch 18: 0.08013663971796632\n",
      "Average loss epoch 19: 0.06996472114697098\n",
      "Average loss epoch 20: 0.0691787117626518\n",
      "Accuracy: 0.8225\n"
     ]
    }
   ],
   "source": [
    "# Step 2: create placeholders for X and Y\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "Y = tf.placeholder(tf.float32, shape=(None, n_classes), name='Y')\n",
    "\n",
    "# Step 3: create weight and bias, initialized to 0\n",
    "W = tf.Variable(tf.zeros([n_features, n_classes]))\n",
    "b = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "# Step 4: build model to predict Y\n",
    "Y_predicted = tf.nn.sigmoid(tf.matmul(X, W) + b)\n",
    "# or\n",
    "# logits = tf.matmul(X, W) + b\n",
    "\n",
    "# Step 5: use cross entropy as the loss function\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(Y_predicted), reduction_indices=1))\n",
    "# or\n",
    "# entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels='Y')\n",
    "# loss = tf.reduce_mean(entropy)\n",
    "\n",
    "\n",
    "# Step 6: using gradient descent with learning rate of 0.001 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "# Step 7: calculate accuracy with test set\n",
    "correct_preds = tf.equal(tf.argmax(Y_predicted, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step 8: initialize the necessary variables, in this case, w and b\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    \n",
    "    # Step 9: train the model\n",
    "    for i in range(1, N_EPOCHS+1): \n",
    "        total_loss = 0\n",
    "        n_batches = int(n_train_samples/BATCH_SIZE)\n",
    "        for j in range(n_batches):\n",
    "            indexes = np.random.choice(n_train_samples, BATCH_SIZE)\n",
    "            X_batch, Y_batch = X_train[indexes], y_train[indexes]\n",
    "            _, loss_batch = sess.run([optimizer, loss], {X: X_batch, Y:Y_batch}) \n",
    "            total_loss += loss_batch\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "    \n",
    "    # Step 10: test the model\n",
    "    print(\"Accuracy:\", accuracy.eval({X: X_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не копипастить этот код для работы с остальными датасетами напишем функцию для обучения логистической регрессии, в которой модель будет обучаться и сохраняться в указанное место:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log_reg(X_train, y_train, epochs, batch_size, learning_rate, n_classes, path_model):\n",
    "    n_features = X_train.shape[1]\n",
    "    n_train_samples = X_train.shape[0]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, n_classes), name='Y')\n",
    "\n",
    "    W = tf.Variable(tf.truncated_normal((n_features, n_classes), name='weights'))\n",
    "    b = tf.Variable(tf.zeros((n_classes)), name='bias')\n",
    "    \n",
    "    logits = tf.matmul(X, W) + b\n",
    "    \n",
    "    if n_classes > 2:\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y, name='loss')\n",
    "    else:\n",
    "        entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "        \n",
    "    loss = tf.reduce_mean(entropy)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    preds = tf.nn.softmax(logits) if n_classes > 2 else tf.nn.sigmoid(logits)\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "    confusion_matrix = tf.confusion_matrix(labels=tf.argmax(Y, 1), predictions=tf.argmax(preds, 1),\n",
    "                                           num_classes=n_classes)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer()) \n",
    "        for i in range(1, epochs+1): \n",
    "            total_loss = 0\n",
    "            n_batches = int(n_train_samples/batch_size)\n",
    "            for j in range(n_batches):\n",
    "                indexes = np.random.choice(n_train_samples, batch_size)\n",
    "                X_batch, Y_batch = X_train[indexes], y_train[indexes]\n",
    "                _, loss_batch = sess.run([optimizer, loss], {X: X_batch, Y:Y_batch}) \n",
    "                total_loss += loss_batch\n",
    "            print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, path_model)\n",
    "        print('Model was saved.')\n",
    "    return X, Y, accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(X_test, y_test, path_model, clf_report=False):\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, path_model)\n",
    "        print('Accuracy for test : {}'.format(accuracy.eval({X: X_test, Y: y_test})))\n",
    "        if clf_report:\n",
    "            conf = confusion_matrix.eval({X: X_test, Y: y_test})\n",
    "            TP = np.diag(conf)\n",
    "            FP = np.sum(conf, axis=0) - TP\n",
    "            FN = np.sum(conf, axis=1) - TP\n",
    "            num_classes = conf.shape[0]\n",
    "            TN = []\n",
    "            for i in range(num_classes):\n",
    "                temp = np.delete(conf, i, 0)    # delete ith row\n",
    "                temp = np.delete(temp, i, 1)  # delete ith column\n",
    "                TN.append(sum(sum(temp)))\n",
    "            precision = TP / (TP + FP)\n",
    "            recall = TP / (TP + FN)\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            return pd.DataFrame([precision, recall, f1], columns=['Precision', 'Recall', 'F1'], index=range(1, num_classes+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 1: 0.9183258205652237\n",
      "Average loss epoch 2: 0.6139610275626183\n",
      "Average loss epoch 3: 0.4838238596916199\n",
      "Average loss epoch 4: 0.4190639529377222\n",
      "Average loss epoch 5: 0.3903151357546449\n",
      "Average loss epoch 6: 0.3544321464374661\n",
      "Average loss epoch 7: 0.34322663079947235\n",
      "Average loss epoch 8: 0.3467656448483467\n",
      "Average loss epoch 9: 0.3091584509238601\n",
      "Average loss epoch 10: 0.309259288944304\n",
      "Average loss epoch 11: 0.3163765277713537\n",
      "Average loss epoch 12: 0.2939495490863919\n",
      "Average loss epoch 13: 0.30257267840206625\n",
      "Average loss epoch 14: 0.2923426557332277\n",
      "Average loss epoch 15: 0.3009375812485814\n",
      "Average loss epoch 16: 0.3027746107429266\n",
      "Average loss epoch 17: 0.2747647317126393\n",
      "Average loss epoch 18: 0.2760469681583345\n",
      "Average loss epoch 19: 0.28648398350924253\n",
      "Average loss epoch 20: 0.26035088673233986\n",
      "Model was saved.\n"
     ]
    }
   ],
   "source": [
    "X, Y, accuracy, confusion_matrix = train_log_reg(X_train, y_train, \n",
    "                                                 epochs=20, batch_size=20,\n",
    "                                                 learning_rate=0.01, n_classes=2,\n",
    "                                                 path_model='./models/simple_clf.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/simple_clf.ckpt\n",
      "Accuracy for test : 0.875\n"
     ]
    }
   ],
   "source": [
    "predict_test(X_test, y_test, './models/simple_clf.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_titanic(X):\n",
    "    scaler = MaxAbsScaler()\n",
    "    X = X.drop(['Name', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'], axis=1)\n",
    "    X = X.dropna()\n",
    "    X['Sex'] = X['Sex'].map({'male': 1, 'female': 0})\n",
    "    X['Age'] = scaler.fit_transform(X['Age'].values.reshape(-1,1))\n",
    "    labels = X['Survived']\n",
    "    return X.drop('Survived', axis=1), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/titanic.csv', index_col=0, sep='\\t')\n",
    "data, labels = preprocess_titanic(data)\n",
    "labels_onehot = pd.get_dummies(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels_onehot, stratify=labels,\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X_train, titanic_X_test, titanic_y_train, titanic_y_test = X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 1: 0.7540777564048767\n",
      "Average loss epoch 2: 0.8711386799812317\n",
      "Average loss epoch 3: 0.8062511086463928\n",
      "Average loss epoch 4: 0.7965465068817139\n",
      "Average loss epoch 5: 0.6868812084197998\n",
      "Average loss epoch 6: 0.6877895951271057\n",
      "Average loss epoch 7: 0.6744686126708984\n",
      "Average loss epoch 8: 0.7392046570777893\n",
      "Average loss epoch 9: 0.7700897336006165\n",
      "Average loss epoch 10: 0.6694975614547729\n",
      "Average loss epoch 11: 0.6656660795211792\n",
      "Average loss epoch 12: 0.6380314588546753\n",
      "Average loss epoch 13: 0.6495214462280273\n",
      "Average loss epoch 14: 0.7071293950080871\n",
      "Average loss epoch 15: 0.6317631125450134\n",
      "Average loss epoch 16: 0.6714646697044373\n",
      "Average loss epoch 17: 0.6456583738327026\n",
      "Average loss epoch 18: 0.5868741929531097\n",
      "Average loss epoch 19: 0.5635051548480987\n",
      "Average loss epoch 20: 0.5814467549324036\n",
      "Average loss epoch 21: 0.6441594004631043\n",
      "Average loss epoch 22: 0.6641549348831177\n",
      "Average loss epoch 23: 0.6003869056701661\n",
      "Average loss epoch 24: 0.5483025908470154\n",
      "Average loss epoch 25: 0.6016868770122528\n",
      "Average loss epoch 26: 0.580443274974823\n",
      "Average loss epoch 27: 0.5037534058094024\n",
      "Average loss epoch 28: 0.533641117811203\n",
      "Average loss epoch 29: 0.5070729970932006\n",
      "Average loss epoch 30: 0.5382811665534973\n",
      "Average loss epoch 31: 0.5617849111557007\n",
      "Average loss epoch 32: 0.594685298204422\n",
      "Average loss epoch 33: 0.5133753418922424\n",
      "Average loss epoch 34: 0.5201641976833343\n",
      "Average loss epoch 35: 0.5705231368541718\n",
      "Average loss epoch 36: 0.6250886797904969\n",
      "Average loss epoch 37: 0.5447294652462006\n",
      "Average loss epoch 38: 0.5338427305221558\n",
      "Average loss epoch 39: 0.5175568640232087\n",
      "Average loss epoch 40: 0.5317105889320374\n",
      "Average loss epoch 41: 0.526866328716278\n",
      "Average loss epoch 42: 0.5382805109024048\n",
      "Average loss epoch 43: 0.5202136874198914\n",
      "Average loss epoch 44: 0.5547129034996032\n",
      "Average loss epoch 45: 0.5385099172592163\n",
      "Average loss epoch 46: 0.5110420167446137\n",
      "Average loss epoch 47: 0.4608390271663666\n",
      "Average loss epoch 48: 0.5289662063121796\n",
      "Average loss epoch 49: 0.5578800916671753\n",
      "Average loss epoch 50: 0.5573928594589234\n",
      "Average loss epoch 51: 0.5236225426197052\n",
      "Average loss epoch 52: 0.5906341910362244\n",
      "Average loss epoch 53: 0.4932103157043457\n",
      "Average loss epoch 54: 0.48770058155059814\n",
      "Average loss epoch 55: 0.4756433546543121\n",
      "Average loss epoch 56: 0.47450136542320254\n",
      "Average loss epoch 57: 0.5765299081802369\n",
      "Average loss epoch 58: 0.5154777884483337\n",
      "Average loss epoch 59: 0.4605884313583374\n",
      "Average loss epoch 60: 0.5284645617008209\n",
      "Average loss epoch 61: 0.4688724160194397\n",
      "Average loss epoch 62: 0.5031469881534576\n",
      "Average loss epoch 63: 0.4685835599899292\n",
      "Average loss epoch 64: 0.5438661098480224\n",
      "Average loss epoch 65: 0.5208057403564453\n",
      "Average loss epoch 66: 0.5282918572425842\n",
      "Average loss epoch 67: 0.4932507336139679\n",
      "Average loss epoch 68: 0.4860823690891266\n",
      "Average loss epoch 69: 0.3876673340797424\n",
      "Average loss epoch 70: 0.578074288368225\n",
      "Average loss epoch 71: 0.5231992542743683\n",
      "Average loss epoch 72: 0.475396329164505\n",
      "Average loss epoch 73: 0.44229320287704466\n",
      "Average loss epoch 74: 0.541338849067688\n",
      "Average loss epoch 75: 0.48486846685409546\n",
      "Average loss epoch 76: 0.5205751180648803\n",
      "Average loss epoch 77: 0.5151759088039398\n",
      "Average loss epoch 78: 0.5262442290782928\n",
      "Average loss epoch 79: 0.544730681180954\n",
      "Average loss epoch 80: 0.4687076389789581\n",
      "Average loss epoch 81: 0.4586367607116699\n",
      "Average loss epoch 82: 0.505620288848877\n",
      "Average loss epoch 83: 0.48361838459968565\n",
      "Average loss epoch 84: 0.4230186760425568\n",
      "Average loss epoch 85: 0.46966648697853086\n",
      "Average loss epoch 86: 0.4627938807010651\n",
      "Average loss epoch 87: 0.4950664103031158\n",
      "Average loss epoch 88: 0.5316763043403625\n",
      "Average loss epoch 89: 0.4492505371570587\n",
      "Average loss epoch 90: 0.49623913764953614\n",
      "Average loss epoch 91: 0.4743707537651062\n",
      "Average loss epoch 92: 0.4512630343437195\n",
      "Average loss epoch 93: 0.5155420660972595\n",
      "Average loss epoch 94: 0.4592648148536682\n",
      "Average loss epoch 95: 0.4269526243209839\n",
      "Average loss epoch 96: 0.46409305930137634\n",
      "Average loss epoch 97: 0.42532594203948976\n",
      "Average loss epoch 98: 0.48276243209838865\n",
      "Average loss epoch 99: 0.4260775327682495\n",
      "Average loss epoch 100: 0.38299205899238586\n",
      "Model was saved.\n"
     ]
    }
   ],
   "source": [
    "X, Y, accuracy, confusion_matrix = train_log_reg(X_train.values, y_train.values, \n",
    "                               epochs=100, batch_size=20,\n",
    "                               learning_rate=0.1, n_classes=2, path_model='./models/titanic.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/titanic.ckpt\n",
      "Accuracy for test : 0.8846153616905212\n"
     ]
    }
   ],
   "source": [
    "predict_test(X_test, y_test, './models/titanic.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Thyroid_Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>on_thyroxine</th>\n",
       "      <th>query_on_thyroxine</th>\n",
       "      <th>sick</th>\n",
       "      <th>pregnant</th>\n",
       "      <th>lithium</th>\n",
       "      <th>goitre</th>\n",
       "      <th>hypopituitary</th>\n",
       "      <th>TSH</th>\n",
       "      <th>T3</th>\n",
       "      <th>TT4</th>\n",
       "      <th>T4U</th>\n",
       "      <th>FTI</th>\n",
       "      <th>ref_SVHC</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.048638</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.611106</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>1.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.211204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.027783</td>\n",
       "      <td>0.118183</td>\n",
       "      <td>-0.064208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.258065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.192620</td>\n",
       "      <td>-0.390416</td>\n",
       "      <td>0.166661</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.561574</td>\n",
       "      <td>-0.130435</td>\n",
       "      <td>1.999994</td>\n",
       "      <td>0.317606</td>\n",
       "      <td>2.038071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.309606</td>\n",
       "      <td>-1.043478</td>\n",
       "      <td>-1.166672</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-1.193548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex  on_thyroxine  query_on_thyroxine  sick  pregnant  lithium  \\\n",
       "0 -0.419355  0.0           0.0                 0.0   0.0       0.0      0.0   \n",
       "1 -1.000000  0.0           0.0                 0.0   0.0       0.0      0.0   \n",
       "2 -0.258065  1.0           0.0                 0.0   0.0       0.0      0.0   \n",
       "3  0.516129  0.0           1.0                 0.0   0.0       0.0      0.0   \n",
       "4  0.516129  0.0           0.0                 0.0   0.0       0.0      0.0   \n",
       "\n",
       "   goitre  hypopituitary       TSH        T3       TT4       T4U       FTI  \\\n",
       "0     0.0            0.0 -0.048638  0.652174  0.611106  0.850000  0.064516   \n",
       "1     0.0            0.0  1.211204  0.000000 -0.027783  0.118183 -0.064208   \n",
       "2     0.0            0.0 -0.192620 -0.390416  0.166661 -0.300000  0.419355   \n",
       "3     0.0            0.0 -0.561574 -0.130435  1.999994  0.317606  2.038071   \n",
       "4     0.0            0.0 -0.309606 -1.043478 -1.166672 -0.500000 -1.193548   \n",
       "\n",
       "   ref_SVHC     Class  \n",
       "0       1.0  negative  \n",
       "1       0.0  negative  \n",
       "2       0.0  negative  \n",
       "3       0.0  negative  \n",
       "4       0.0  negative  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/preprocessed_Thyroid_Disease.csv', index_col=0, na_values='?')\n",
    "df = df[df['Class'] != 'secondary_hypothyroid']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['Class']\n",
    "labels_onehot = pd.get_dummies(labels)\n",
    "df.drop('Class', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, labels_onehot, stratify=labels,\n",
    "                                                    test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "TD_X_train, TD_X_test, TD_y_train, TD_y_test = X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 1: 1.0582122210915208\n",
      "Average loss epoch 2: 0.5835154990777902\n",
      "Average loss epoch 3: 0.4278638374509541\n",
      "Average loss epoch 4: 0.35535573790259395\n",
      "Average loss epoch 5: 0.3190276506522023\n",
      "Average loss epoch 6: 0.27588453302358057\n",
      "Average loss epoch 7: 0.2743888726154118\n",
      "Average loss epoch 8: 0.2530796428521474\n",
      "Average loss epoch 9: 0.24477253965240844\n",
      "Average loss epoch 10: 0.212296049672027\n",
      "Average loss epoch 11: 0.2357821928218324\n",
      "Average loss epoch 12: 0.24287026666157635\n",
      "Average loss epoch 13: 0.22537983134917333\n",
      "Average loss epoch 14: 0.2722994661658791\n",
      "Average loss epoch 15: 0.23018625597898842\n",
      "Average loss epoch 16: 0.21168142174046936\n",
      "Average loss epoch 17: 0.21367657398606868\n",
      "Average loss epoch 18: 0.21825344287228923\n",
      "Average loss epoch 19: 0.19981994855065718\n",
      "Average loss epoch 20: 0.23675515367946726\n",
      "Model was saved.\n"
     ]
    }
   ],
   "source": [
    "X, Y, accuracy, confusion_matrix = train_log_reg(X_train.values, y_train.values, \n",
    "                               epochs=20, batch_size=20,\n",
    "                               learning_rate=0.01, n_classes=3, path_model='./models/TD.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/TD.ckpt\n",
      "Accuracy for test : 0.9363732933998108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data_analyst/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.948352</td>\n",
       "      <td>0.606061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.990815</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969118</td>\n",
       "      <td>0.701754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision    Recall        F1\n",
       "1        NaN  0.948352  0.606061\n",
       "2        0.0  0.990815  0.833333\n",
       "3        NaN  0.969118  0.701754"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test(X_test, y_test, './models/TD.ckpt', clf_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Keras:\n",
    "Проделаем тоже самое в Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 1s 410us/step - loss: 0.5631 - acc: 0.6644 - val_loss: 0.4906 - val_acc: 0.7438\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 0s 307us/step - loss: 0.4137 - acc: 0.8100 - val_loss: 0.4136 - val_acc: 0.8212\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 0s 170us/step - loss: 0.3564 - acc: 0.8678 - val_loss: 0.3805 - val_acc: 0.8387\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 0s 151us/step - loss: 0.3294 - acc: 0.8822 - val_loss: 0.3625 - val_acc: 0.8400\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 0s 147us/step - loss: 0.3137 - acc: 0.8847 - val_loss: 0.3511 - val_acc: 0.8412\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 0s 150us/step - loss: 0.3033 - acc: 0.8897 - val_loss: 0.3432 - val_acc: 0.8512\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 0s 159us/step - loss: 0.2958 - acc: 0.8928 - val_loss: 0.3370 - val_acc: 0.8550\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 0s 148us/step - loss: 0.2899 - acc: 0.8966 - val_loss: 0.3322 - val_acc: 0.8637\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 0s 150us/step - loss: 0.2852 - acc: 0.8988 - val_loss: 0.3282 - val_acc: 0.8700\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 1s 327us/step - loss: 0.2813 - acc: 0.9000 - val_loss: 0.3248 - val_acc: 0.8687\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 1s 317us/step - loss: 0.2779 - acc: 0.9025 - val_loss: 0.3218 - val_acc: 0.8700\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 0s 176us/step - loss: 0.2749 - acc: 0.9031 - val_loss: 0.3191 - val_acc: 0.8750\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 0s 155us/step - loss: 0.2723 - acc: 0.9022 - val_loss: 0.3168 - val_acc: 0.8775\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 0s 149us/step - loss: 0.2700 - acc: 0.9034 - val_loss: 0.3147 - val_acc: 0.8775\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 0s 152us/step - loss: 0.2679 - acc: 0.9047 - val_loss: 0.3127 - val_acc: 0.8800\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 0s 156us/step - loss: 0.2660 - acc: 0.9044 - val_loss: 0.3110 - val_acc: 0.8800\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 0s 154us/step - loss: 0.2642 - acc: 0.9053 - val_loss: 0.3093 - val_acc: 0.8812\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 0s 166us/step - loss: 0.2626 - acc: 0.9041 - val_loss: 0.3078 - val_acc: 0.8837\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 1s 350us/step - loss: 0.2611 - acc: 0.9044 - val_loss: 0.3064 - val_acc: 0.8837\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 1s 328us/step - loss: 0.2598 - acc: 0.9050 - val_loss: 0.3052 - val_acc: 0.8825\n"
     ]
    }
   ],
   "source": [
    "log_reg = Sequential()\n",
    "log_reg.add(Dense(2, input_dim=5, activation='sigmoid'))\n",
    "log_reg.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "hist = log_reg.fit(rand_clf_X_train, rand_clf_y_train, batch_size=20, epochs=20, verbose=1,\n",
    "            validation_data=(rand_clf_X_test, rand_clf_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test: 0.8824999958276749\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for test: {}'.format(hist.history['val_acc'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 26 samples\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.7800 - acc: 0.6150 - val_loss: 0.6510 - val_acc: 0.7308\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 142us/step - loss: 0.7764 - acc: 0.6150 - val_loss: 0.6489 - val_acc: 0.7308\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 142us/step - loss: 0.7730 - acc: 0.6150 - val_loss: 0.6469 - val_acc: 0.7308\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 146us/step - loss: 0.7696 - acc: 0.6200 - val_loss: 0.6449 - val_acc: 0.7115\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 174us/step - loss: 0.7663 - acc: 0.6200 - val_loss: 0.6430 - val_acc: 0.7115\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 183us/step - loss: 0.7634 - acc: 0.6200 - val_loss: 0.6412 - val_acc: 0.7115\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 188us/step - loss: 0.7607 - acc: 0.6200 - val_loss: 0.6394 - val_acc: 0.7115\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 199us/step - loss: 0.7574 - acc: 0.6200 - val_loss: 0.6377 - val_acc: 0.7115\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 228us/step - loss: 0.7542 - acc: 0.6200 - val_loss: 0.6360 - val_acc: 0.7115\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 206us/step - loss: 0.7513 - acc: 0.6200 - val_loss: 0.6343 - val_acc: 0.7115\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 196us/step - loss: 0.7485 - acc: 0.6200 - val_loss: 0.6328 - val_acc: 0.7115\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 193us/step - loss: 0.7461 - acc: 0.6200 - val_loss: 0.6312 - val_acc: 0.7115\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 172us/step - loss: 0.7432 - acc: 0.6200 - val_loss: 0.6297 - val_acc: 0.7115\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 200us/step - loss: 0.7406 - acc: 0.6150 - val_loss: 0.6283 - val_acc: 0.6923\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 200us/step - loss: 0.7386 - acc: 0.6150 - val_loss: 0.6269 - val_acc: 0.6923\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 182us/step - loss: 0.7358 - acc: 0.6150 - val_loss: 0.6255 - val_acc: 0.6731\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 183us/step - loss: 0.7334 - acc: 0.6150 - val_loss: 0.6241 - val_acc: 0.6731\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 207us/step - loss: 0.7312 - acc: 0.6150 - val_loss: 0.6228 - val_acc: 0.6731\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 181us/step - loss: 0.7290 - acc: 0.6150 - val_loss: 0.6216 - val_acc: 0.6731\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 178us/step - loss: 0.7270 - acc: 0.6150 - val_loss: 0.6203 - val_acc: 0.6731\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 163us/step - loss: 0.7252 - acc: 0.6150 - val_loss: 0.6191 - val_acc: 0.6731\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 166us/step - loss: 0.7227 - acc: 0.6150 - val_loss: 0.6180 - val_acc: 0.6731\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 134us/step - loss: 0.7206 - acc: 0.6150 - val_loss: 0.6168 - val_acc: 0.6731\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 152us/step - loss: 0.7186 - acc: 0.6150 - val_loss: 0.6157 - val_acc: 0.6731\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 187us/step - loss: 0.7168 - acc: 0.6150 - val_loss: 0.6146 - val_acc: 0.6731\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 187us/step - loss: 0.7150 - acc: 0.6150 - val_loss: 0.6135 - val_acc: 0.6731\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 164us/step - loss: 0.7130 - acc: 0.6150 - val_loss: 0.6125 - val_acc: 0.6923\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.9413 - acc: 0.500 - 0s 178us/step - loss: 0.7118 - acc: 0.6150 - val_loss: 0.6115 - val_acc: 0.6923\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 168us/step - loss: 0.7095 - acc: 0.6150 - val_loss: 0.6105 - val_acc: 0.6923\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 173us/step - loss: 0.7079 - acc: 0.6150 - val_loss: 0.6095 - val_acc: 0.6923\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 168us/step - loss: 0.7061 - acc: 0.6150 - val_loss: 0.6086 - val_acc: 0.6923\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 172us/step - loss: 0.7047 - acc: 0.6100 - val_loss: 0.6076 - val_acc: 0.6923\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 190us/step - loss: 0.7029 - acc: 0.6100 - val_loss: 0.6067 - val_acc: 0.6923\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 173us/step - loss: 0.7012 - acc: 0.6100 - val_loss: 0.6058 - val_acc: 0.6923\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 174us/step - loss: 0.6997 - acc: 0.6100 - val_loss: 0.6049 - val_acc: 0.6923\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 161us/step - loss: 0.6982 - acc: 0.6100 - val_loss: 0.6041 - val_acc: 0.6923\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 166us/step - loss: 0.6967 - acc: 0.6100 - val_loss: 0.6032 - val_acc: 0.6923\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 143us/step - loss: 0.6954 - acc: 0.6100 - val_loss: 0.6024 - val_acc: 0.6923\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 180us/step - loss: 0.6941 - acc: 0.6100 - val_loss: 0.6015 - val_acc: 0.6923\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 193us/step - loss: 0.6925 - acc: 0.6100 - val_loss: 0.6007 - val_acc: 0.6923\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 177us/step - loss: 0.6914 - acc: 0.6100 - val_loss: 0.5999 - val_acc: 0.6923\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 158us/step - loss: 0.6898 - acc: 0.6100 - val_loss: 0.5992 - val_acc: 0.6923\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 137us/step - loss: 0.6882 - acc: 0.6100 - val_loss: 0.5984 - val_acc: 0.6923\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 165us/step - loss: 0.6869 - acc: 0.6100 - val_loss: 0.5976 - val_acc: 0.6923\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 183us/step - loss: 0.6858 - acc: 0.6100 - val_loss: 0.5969 - val_acc: 0.6923\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 190us/step - loss: 0.6843 - acc: 0.6100 - val_loss: 0.5961 - val_acc: 0.6923\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 173us/step - loss: 0.6831 - acc: 0.6100 - val_loss: 0.5954 - val_acc: 0.6923\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5051 - acc: 0.800 - 0s 209us/step - loss: 0.6817 - acc: 0.6100 - val_loss: 0.5947 - val_acc: 0.6923\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 176us/step - loss: 0.6805 - acc: 0.6100 - val_loss: 0.5940 - val_acc: 0.6923\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 202us/step - loss: 0.6796 - acc: 0.6100 - val_loss: 0.5933 - val_acc: 0.6923\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 177us/step - loss: 0.6782 - acc: 0.6100 - val_loss: 0.5926 - val_acc: 0.6923\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 168us/step - loss: 0.6769 - acc: 0.6100 - val_loss: 0.5919 - val_acc: 0.6923\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 177us/step - loss: 0.6758 - acc: 0.6100 - val_loss: 0.5912 - val_acc: 0.6923\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 169us/step - loss: 0.6749 - acc: 0.6100 - val_loss: 0.5905 - val_acc: 0.6923\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 182us/step - loss: 0.6735 - acc: 0.6150 - val_loss: 0.5899 - val_acc: 0.6923\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 178us/step - loss: 0.6723 - acc: 0.6150 - val_loss: 0.5892 - val_acc: 0.6923\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 175us/step - loss: 0.6712 - acc: 0.6150 - val_loss: 0.5886 - val_acc: 0.6923\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 181us/step - loss: 0.6701 - acc: 0.6200 - val_loss: 0.5879 - val_acc: 0.6923\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 174us/step - loss: 0.6692 - acc: 0.6200 - val_loss: 0.5873 - val_acc: 0.6923\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 213us/step - loss: 0.6681 - acc: 0.6200 - val_loss: 0.5867 - val_acc: 0.6923\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 175us/step - loss: 0.6670 - acc: 0.6200 - val_loss: 0.5861 - val_acc: 0.6923\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 160us/step - loss: 0.6658 - acc: 0.6200 - val_loss: 0.5855 - val_acc: 0.6923\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 159us/step - loss: 0.6648 - acc: 0.6200 - val_loss: 0.5849 - val_acc: 0.7115\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 157us/step - loss: 0.6637 - acc: 0.6200 - val_loss: 0.5843 - val_acc: 0.7115\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 182us/step - loss: 0.6626 - acc: 0.6200 - val_loss: 0.5837 - val_acc: 0.7115\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 191us/step - loss: 0.6616 - acc: 0.6200 - val_loss: 0.5831 - val_acc: 0.7115\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 177us/step - loss: 0.6606 - acc: 0.6200 - val_loss: 0.5825 - val_acc: 0.7308\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 177us/step - loss: 0.6598 - acc: 0.6200 - val_loss: 0.5819 - val_acc: 0.7308\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 162us/step - loss: 0.6589 - acc: 0.6200 - val_loss: 0.5813 - val_acc: 0.7308\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 172us/step - loss: 0.6580 - acc: 0.6200 - val_loss: 0.5807 - val_acc: 0.7308\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 177us/step - loss: 0.6568 - acc: 0.6200 - val_loss: 0.5802 - val_acc: 0.7308\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 168us/step - loss: 0.6560 - acc: 0.6200 - val_loss: 0.5796 - val_acc: 0.7308\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 176us/step - loss: 0.6550 - acc: 0.6200 - val_loss: 0.5791 - val_acc: 0.7308\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 180us/step - loss: 0.6543 - acc: 0.6200 - val_loss: 0.5785 - val_acc: 0.7308\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 169us/step - loss: 0.6530 - acc: 0.6200 - val_loss: 0.5780 - val_acc: 0.7308\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 181us/step - loss: 0.6522 - acc: 0.6200 - val_loss: 0.5774 - val_acc: 0.7308\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 209us/step - loss: 0.6511 - acc: 0.6200 - val_loss: 0.5769 - val_acc: 0.7308\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 179us/step - loss: 0.6504 - acc: 0.6200 - val_loss: 0.5763 - val_acc: 0.7308\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 160us/step - loss: 0.6494 - acc: 0.6200 - val_loss: 0.5758 - val_acc: 0.7308\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 451us/step - loss: 0.6485 - acc: 0.6200 - val_loss: 0.5753 - val_acc: 0.7308\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 239us/step - loss: 0.6479 - acc: 0.6150 - val_loss: 0.5748 - val_acc: 0.7308\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 599us/step - loss: 0.6468 - acc: 0.6200 - val_loss: 0.5743 - val_acc: 0.7308\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 203us/step - loss: 0.6462 - acc: 0.6150 - val_loss: 0.5737 - val_acc: 0.7308\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 392us/step - loss: 0.6451 - acc: 0.6150 - val_loss: 0.5732 - val_acc: 0.7500\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 602us/step - loss: 0.6443 - acc: 0.6150 - val_loss: 0.5727 - val_acc: 0.7500\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 148us/step - loss: 0.6433 - acc: 0.6150 - val_loss: 0.5722 - val_acc: 0.7500\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 418us/step - loss: 0.6426 - acc: 0.6150 - val_loss: 0.5717 - val_acc: 0.7500\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 537us/step - loss: 0.6417 - acc: 0.6150 - val_loss: 0.5712 - val_acc: 0.7500\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 330us/step - loss: 0.6410 - acc: 0.6150 - val_loss: 0.5707 - val_acc: 0.7500\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 584us/step - loss: 0.6400 - acc: 0.6150 - val_loss: 0.5702 - val_acc: 0.7500\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 462us/step - loss: 0.6392 - acc: 0.6150 - val_loss: 0.5698 - val_acc: 0.7500\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 499us/step - loss: 0.6385 - acc: 0.6200 - val_loss: 0.5693 - val_acc: 0.7500\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 257us/step - loss: 0.6376 - acc: 0.6200 - val_loss: 0.5688 - val_acc: 0.7500\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 459us/step - loss: 0.6369 - acc: 0.6200 - val_loss: 0.5683 - val_acc: 0.7500\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 399us/step - loss: 0.6360 - acc: 0.6200 - val_loss: 0.5678 - val_acc: 0.7500\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 503us/step - loss: 0.6353 - acc: 0.6200 - val_loss: 0.5674 - val_acc: 0.7500\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 391us/step - loss: 0.6344 - acc: 0.6200 - val_loss: 0.5669 - val_acc: 0.7500\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 573us/step - loss: 0.6337 - acc: 0.6200 - val_loss: 0.5664 - val_acc: 0.7500\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 249us/step - loss: 0.6330 - acc: 0.6200 - val_loss: 0.5660 - val_acc: 0.7500\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 480us/step - loss: 0.6323 - acc: 0.6200 - val_loss: 0.5655 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "log_reg = Sequential()\n",
    "log_reg.add(Dense(2, input_dim=4, activation='sigmoid'))\n",
    "log_reg.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "hist = log_reg.fit(titanic_X_train, titanic_y_train, batch_size=20, epochs=100, verbose=1,\n",
    "            validation_data=(titanic_X_test, titanic_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test: 0.749999986245082\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for test: {}'.format(hist.history['val_acc'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Thyroid_Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@as_keras_metric\n",
    "def auc_pr(y_true, y_pred, curve='PR'):\n",
    "    return tf.metrics.auc(y_true, y_pred, curve=curve, summation_method='careful_interpolation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2827 samples, validate on 943 samples\n",
      "Epoch 1/20\n",
      "2827/2827 [==============================] - 1s 357us/step - loss: 0.8727 - auc_pr: 0.4739 - val_loss: 0.6228 - val_auc_pr: 0.6845\n",
      "Epoch 2/20\n",
      "2827/2827 [==============================] - 1s 337us/step - loss: 0.5024 - auc_pr: 0.7780 - val_loss: 0.4224 - val_auc_pr: 0.8405\n",
      "Epoch 3/20\n",
      "2827/2827 [==============================] - 1s 219us/step - loss: 0.3725 - auc_pr: 0.8728 - val_loss: 0.3543 - val_auc_pr: 0.8946\n",
      "Epoch 4/20\n",
      "2827/2827 [==============================] - 1s 218us/step - loss: 0.3140 - auc_pr: 0.9076 - val_loss: 0.3147 - val_auc_pr: 0.9182\n",
      "Epoch 5/20\n",
      "2827/2827 [==============================] - 1s 358us/step - loss: 0.2852 - auc_pr: 0.9262 - val_loss: 0.2834 - val_auc_pr: 0.9316\n",
      "Epoch 6/20\n",
      "2827/2827 [==============================] - 1s 265us/step - loss: 0.2656 - auc_pr: 0.9362 - val_loss: 0.2761 - val_auc_pr: 0.9370\n",
      "Epoch 7/20\n",
      "2827/2827 [==============================] - 1s 227us/step - loss: 0.2571 - auc_pr: 0.9430 - val_loss: 0.2727 - val_auc_pr: 0.9459\n",
      "Epoch 8/20\n",
      "2827/2827 [==============================] - 1s 212us/step - loss: 0.2456 - auc_pr: 0.9482 - val_loss: 0.2626 - val_auc_pr: 0.9503\n",
      "Epoch 9/20\n",
      "2827/2827 [==============================] - 1s 388us/step - loss: 0.2389 - auc_pr: 0.9520 - val_loss: 0.2592 - val_auc_pr: 0.9534\n",
      "Epoch 10/20\n",
      "2827/2827 [==============================] - 1s 230us/step - loss: 0.2329 - auc_pr: 0.9547 - val_loss: 0.2544 - val_auc_pr: 0.9560\n",
      "Epoch 11/20\n",
      "2827/2827 [==============================] - 1s 217us/step - loss: 0.2262 - auc_pr: 0.9574 - val_loss: 0.2685 - val_auc_pr: 0.9586\n",
      "Epoch 12/20\n",
      "2827/2827 [==============================] - 1s 226us/step - loss: 0.2232 - auc_pr: 0.9597 - val_loss: 0.2496 - val_auc_pr: 0.9608\n",
      "Epoch 13/20\n",
      "2827/2827 [==============================] - 1s 392us/step - loss: 0.2189 - auc_pr: 0.9619 - val_loss: 0.2490 - val_auc_pr: 0.9629\n",
      "Epoch 14/20\n",
      "2827/2827 [==============================] - 1s 226us/step - loss: 0.2159 - auc_pr: 0.9639 - val_loss: 0.2465 - val_auc_pr: 0.9647\n",
      "Epoch 15/20\n",
      "2827/2827 [==============================] - 1s 216us/step - loss: 0.2135 - auc_pr: 0.9655 - val_loss: 0.2514 - val_auc_pr: 0.9663\n",
      "Epoch 16/20\n",
      "2827/2827 [==============================] - 1s 250us/step - loss: 0.2110 - auc_pr: 0.9671 - val_loss: 0.2420 - val_auc_pr: 0.9677\n",
      "Epoch 17/20\n",
      "2827/2827 [==============================] - 1s 378us/step - loss: 0.2094 - auc_pr: 0.9683 - val_loss: 0.2416 - val_auc_pr: 0.9689\n",
      "Epoch 18/20\n",
      "2827/2827 [==============================] - 1s 218us/step - loss: 0.2075 - auc_pr: 0.9695 - val_loss: 0.2406 - val_auc_pr: 0.9700\n",
      "Epoch 19/20\n",
      "2827/2827 [==============================] - 1s 224us/step - loss: 0.2056 - auc_pr: 0.9705 - val_loss: 0.2394 - val_auc_pr: 0.9709\n",
      "Epoch 20/20\n",
      "2827/2827 [==============================] - 1s 289us/step - loss: 0.2045 - auc_pr: 0.9713 - val_loss: 0.2387 - val_auc_pr: 0.9717\n"
     ]
    }
   ],
   "source": [
    "log_reg = Sequential()\n",
    "log_reg.add(Dense(3, input_dim=15, activation='softmax'))\n",
    "log_reg.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[auc_pr])\n",
    "\n",
    "hist = log_reg.fit(TD_X_train, TD_y_train, batch_size=20, epochs=20, verbose=1,\n",
    "            validation_data=(TD_X_test, TD_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR_auc for test: 0.9717485174015243\n"
     ]
    }
   ],
   "source": [
    "print('PR_auc for test: {}'.format(hist.history['val_auc_pr'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "scaler = RobustScaler()\n",
    "boston = load_boston()\n",
    "X = scaler.fit_transform(boston.data)\n",
    "y = boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/20\n",
      "379/379 [==============================] - 0s 1ms/step - loss: 267.1233 - val_loss: 126.3030\n",
      "Epoch 2/20\n",
      "379/379 [==============================] - 0s 521us/step - loss: 104.8280 - val_loss: 67.2553\n",
      "Epoch 3/20\n",
      "379/379 [==============================] - 0s 337us/step - loss: 59.1381 - val_loss: 46.8592\n",
      "Epoch 4/20\n",
      "379/379 [==============================] - 0s 327us/step - loss: 43.4062 - val_loss: 34.2825\n",
      "Epoch 5/20\n",
      "379/379 [==============================] - 0s 301us/step - loss: 34.9173 - val_loss: 33.2025\n",
      "Epoch 6/20\n",
      "379/379 [==============================] - 0s 304us/step - loss: 30.6958 - val_loss: 35.1250\n",
      "Epoch 7/20\n",
      "379/379 [==============================] - 0s 281us/step - loss: 30.4011 - val_loss: 27.9073\n",
      "Epoch 8/20\n",
      "379/379 [==============================] - 0s 279us/step - loss: 29.4844 - val_loss: 30.4638\n",
      "Epoch 9/20\n",
      "379/379 [==============================] - 0s 280us/step - loss: 27.7331 - val_loss: 27.8862\n",
      "Epoch 10/20\n",
      "379/379 [==============================] - 0s 278us/step - loss: 27.0659 - val_loss: 26.9953\n",
      "Epoch 11/20\n",
      "379/379 [==============================] - 0s 286us/step - loss: 27.2541 - val_loss: 26.7292\n",
      "Epoch 12/20\n",
      "379/379 [==============================] - 0s 260us/step - loss: 25.7051 - val_loss: 25.4469\n",
      "Epoch 13/20\n",
      "379/379 [==============================] - 0s 285us/step - loss: 26.1302 - val_loss: 30.1161\n",
      "Epoch 14/20\n",
      "379/379 [==============================] - 0s 289us/step - loss: 25.0454 - val_loss: 23.5903\n",
      "Epoch 15/20\n",
      "379/379 [==============================] - 0s 292us/step - loss: 25.8840 - val_loss: 30.7076\n",
      "Epoch 16/20\n",
      "379/379 [==============================] - 0s 263us/step - loss: 24.6609 - val_loss: 26.0418\n",
      "Epoch 17/20\n",
      "379/379 [==============================] - 0s 283us/step - loss: 24.6485 - val_loss: 23.2702\n",
      "Epoch 18/20\n",
      "379/379 [==============================] - 0s 285us/step - loss: 24.9336 - val_loss: 31.6495\n",
      "Epoch 19/20\n",
      "379/379 [==============================] - 0s 295us/step - loss: 24.7292 - val_loss: 22.6056\n",
      "Epoch 20/20\n",
      "379/379 [==============================] - 0s 512us/step - loss: 25.2922 - val_loss: 28.3019\n"
     ]
    }
   ],
   "source": [
    "regression = Sequential()\n",
    "# regression.add(Dense(13, input_dim=13, activation='relu'))\n",
    "regression.add(Dense(1, input_dim=13))\n",
    "regression.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "hist = regression.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1,\n",
    "                      validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for test (1 neuron): 28.301932676570623\n"
     ]
    }
   ],
   "source": [
    "print('MSE for test (1 neuron): {}'.format(hist.history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for test (with hidden layer): 18.329604409811065\n"
     ]
    }
   ],
   "source": [
    "print('MSE for test (with hidden layer): {}'.format(hist.history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Titanic with multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline который был выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 571 samples, validate on 143 samples\n",
      "Epoch 1/100\n",
      "571/571 [==============================] - 0s 683us/step - loss: 1.3113 - acc: 0.4247 - val_loss: 1.1843 - val_acc: 0.4301\n",
      "Epoch 2/100\n",
      "571/571 [==============================] - 0s 107us/step - loss: 1.1136 - acc: 0.4536 - val_loss: 1.0140 - val_acc: 0.4441\n",
      "Epoch 3/100\n",
      "571/571 [==============================] - 0s 101us/step - loss: 0.9647 - acc: 0.5114 - val_loss: 0.8892 - val_acc: 0.5105\n",
      "Epoch 4/100\n",
      "571/571 [==============================] - 0s 89us/step - loss: 0.8590 - acc: 0.5236 - val_loss: 0.8045 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "571/571 [==============================] - 0s 93us/step - loss: 0.7889 - acc: 0.5166 - val_loss: 0.7505 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "571/571 [==============================] - 0s 93us/step - loss: 0.7448 - acc: 0.4965 - val_loss: 0.7167 - val_acc: 0.5070\n",
      "Epoch 7/100\n",
      "571/571 [==============================] - 0s 98us/step - loss: 0.7167 - acc: 0.4860 - val_loss: 0.6950 - val_acc: 0.5105\n",
      "Epoch 8/100\n",
      "571/571 [==============================] - 0s 94us/step - loss: 0.6985 - acc: 0.5166 - val_loss: 0.6806 - val_acc: 0.5315\n",
      "Epoch 9/100\n",
      "571/571 [==============================] - 0s 91us/step - loss: 0.6861 - acc: 0.5657 - val_loss: 0.6710 - val_acc: 0.5909\n",
      "Epoch 10/100\n",
      "571/571 [==============================] - 0s 67us/step - loss: 0.6777 - acc: 0.5867 - val_loss: 0.6643 - val_acc: 0.5944\n",
      "Epoch 11/100\n",
      "571/571 [==============================] - 0s 92us/step - loss: 0.6714 - acc: 0.5911 - val_loss: 0.6588 - val_acc: 0.5944\n",
      "Epoch 12/100\n",
      "571/571 [==============================] - 0s 89us/step - loss: 0.6665 - acc: 0.6007 - val_loss: 0.6548 - val_acc: 0.5944\n",
      "Epoch 13/100\n",
      "571/571 [==============================] - 0s 77us/step - loss: 0.6623 - acc: 0.6016 - val_loss: 0.6513 - val_acc: 0.5979\n",
      "Epoch 14/100\n",
      "571/571 [==============================] - 0s 101us/step - loss: 0.6588 - acc: 0.6025 - val_loss: 0.6483 - val_acc: 0.6049\n",
      "Epoch 15/100\n",
      "571/571 [==============================] - 0s 101us/step - loss: 0.6557 - acc: 0.6042 - val_loss: 0.6458 - val_acc: 0.6119\n",
      "Epoch 16/100\n",
      "571/571 [==============================] - 0s 107us/step - loss: 0.6530 - acc: 0.6077 - val_loss: 0.6435 - val_acc: 0.6189\n",
      "Epoch 17/100\n",
      "571/571 [==============================] - 0s 117us/step - loss: 0.6505 - acc: 0.6103 - val_loss: 0.6412 - val_acc: 0.6224\n",
      "Epoch 18/100\n",
      "571/571 [==============================] - 0s 100us/step - loss: 0.6479 - acc: 0.6121 - val_loss: 0.6391 - val_acc: 0.6259\n",
      "Epoch 19/100\n",
      "571/571 [==============================] - 0s 78us/step - loss: 0.6457 - acc: 0.6156 - val_loss: 0.6370 - val_acc: 0.6259\n",
      "Epoch 20/100\n",
      "571/571 [==============================] - 0s 84us/step - loss: 0.6434 - acc: 0.6165 - val_loss: 0.6350 - val_acc: 0.6259\n",
      "Epoch 21/100\n",
      "571/571 [==============================] - 0s 83us/step - loss: 0.6412 - acc: 0.6147 - val_loss: 0.6331 - val_acc: 0.6259\n",
      "Epoch 22/100\n",
      "571/571 [==============================] - 0s 76us/step - loss: 0.6391 - acc: 0.6112 - val_loss: 0.6313 - val_acc: 0.6294\n",
      "Epoch 23/100\n",
      "571/571 [==============================] - 0s 82us/step - loss: 0.6372 - acc: 0.6112 - val_loss: 0.6295 - val_acc: 0.6294\n",
      "Epoch 24/100\n",
      "571/571 [==============================] - 0s 98us/step - loss: 0.6352 - acc: 0.6112 - val_loss: 0.6277 - val_acc: 0.6329\n",
      "Epoch 25/100\n",
      "571/571 [==============================] - 0s 73us/step - loss: 0.6333 - acc: 0.6121 - val_loss: 0.6260 - val_acc: 0.6329\n",
      "Epoch 26/100\n",
      "571/571 [==============================] - 0s 114us/step - loss: 0.6314 - acc: 0.6121 - val_loss: 0.6243 - val_acc: 0.6364\n",
      "Epoch 27/100\n",
      "571/571 [==============================] - 0s 108us/step - loss: 0.6296 - acc: 0.6112 - val_loss: 0.6227 - val_acc: 0.6364\n",
      "Epoch 28/100\n",
      "571/571 [==============================] - 0s 139us/step - loss: 0.6279 - acc: 0.6147 - val_loss: 0.6211 - val_acc: 0.6364\n",
      "Epoch 29/100\n",
      "571/571 [==============================] - 0s 91us/step - loss: 0.6260 - acc: 0.6147 - val_loss: 0.6195 - val_acc: 0.6399\n",
      "Epoch 30/100\n",
      "571/571 [==============================] - 0s 63us/step - loss: 0.6244 - acc: 0.6138 - val_loss: 0.6180 - val_acc: 0.6399\n",
      "Epoch 31/100\n",
      "571/571 [==============================] - 0s 88us/step - loss: 0.6228 - acc: 0.6130 - val_loss: 0.6165 - val_acc: 0.6399\n",
      "Epoch 32/100\n",
      "571/571 [==============================] - 0s 80us/step - loss: 0.6212 - acc: 0.6138 - val_loss: 0.6151 - val_acc: 0.6399\n",
      "Epoch 33/100\n",
      "571/571 [==============================] - 0s 96us/step - loss: 0.6197 - acc: 0.6156 - val_loss: 0.6136 - val_acc: 0.6399\n",
      "Epoch 34/100\n",
      "571/571 [==============================] - 0s 100us/step - loss: 0.6182 - acc: 0.6173 - val_loss: 0.6123 - val_acc: 0.6399\n",
      "Epoch 35/100\n",
      "571/571 [==============================] - 0s 93us/step - loss: 0.6166 - acc: 0.6182 - val_loss: 0.6109 - val_acc: 0.6434\n",
      "Epoch 36/100\n",
      "571/571 [==============================] - 0s 95us/step - loss: 0.6152 - acc: 0.6200 - val_loss: 0.6096 - val_acc: 0.6469\n",
      "Epoch 37/100\n",
      "571/571 [==============================] - 0s 83us/step - loss: 0.6138 - acc: 0.6226 - val_loss: 0.6083 - val_acc: 0.6469\n",
      "Epoch 38/100\n",
      "571/571 [==============================] - 0s 88us/step - loss: 0.6124 - acc: 0.6243 - val_loss: 0.6070 - val_acc: 0.6503\n",
      "Epoch 39/100\n",
      "571/571 [==============================] - 0s 85us/step - loss: 0.6111 - acc: 0.6278 - val_loss: 0.6057 - val_acc: 0.6503\n",
      "Epoch 40/100\n",
      "571/571 [==============================] - 0s 88us/step - loss: 0.6097 - acc: 0.6270 - val_loss: 0.6045 - val_acc: 0.6538\n",
      "Epoch 41/100\n",
      "571/571 [==============================] - 0s 77us/step - loss: 0.6084 - acc: 0.6278 - val_loss: 0.6032 - val_acc: 0.6538\n",
      "Epoch 42/100\n",
      "571/571 [==============================] - 0s 69us/step - loss: 0.6072 - acc: 0.6270 - val_loss: 0.6020 - val_acc: 0.6538\n",
      "Epoch 43/100\n",
      "571/571 [==============================] - 0s 84us/step - loss: 0.6059 - acc: 0.6305 - val_loss: 0.6009 - val_acc: 0.6538\n",
      "Epoch 44/100\n",
      "571/571 [==============================] - 0s 82us/step - loss: 0.6046 - acc: 0.6322 - val_loss: 0.5998 - val_acc: 0.6538\n",
      "Epoch 45/100\n",
      "571/571 [==============================] - 0s 83us/step - loss: 0.6036 - acc: 0.6331 - val_loss: 0.5986 - val_acc: 0.6643\n",
      "Epoch 46/100\n",
      "571/571 [==============================] - 0s 78us/step - loss: 0.6024 - acc: 0.6349 - val_loss: 0.5975 - val_acc: 0.6713\n",
      "Epoch 47/100\n",
      "571/571 [==============================] - 0s 84us/step - loss: 0.6013 - acc: 0.6366 - val_loss: 0.5965 - val_acc: 0.6713\n",
      "Epoch 48/100\n",
      "571/571 [==============================] - 0s 91us/step - loss: 0.6001 - acc: 0.6410 - val_loss: 0.5954 - val_acc: 0.6783\n",
      "Epoch 49/100\n",
      "571/571 [==============================] - 0s 89us/step - loss: 0.5991 - acc: 0.6462 - val_loss: 0.5944 - val_acc: 0.6818\n",
      "Epoch 50/100\n",
      "571/571 [==============================] - 0s 84us/step - loss: 0.5980 - acc: 0.6515 - val_loss: 0.5933 - val_acc: 0.6888\n",
      "Epoch 51/100\n",
      "571/571 [==============================] - 0s 84us/step - loss: 0.5970 - acc: 0.6506 - val_loss: 0.5923 - val_acc: 0.6853\n",
      "Epoch 52/100\n",
      "571/571 [==============================] - 0s 74us/step - loss: 0.5959 - acc: 0.6550 - val_loss: 0.5914 - val_acc: 0.6853\n",
      "Epoch 53/100\n",
      "571/571 [==============================] - 0s 81us/step - loss: 0.5949 - acc: 0.6611 - val_loss: 0.5904 - val_acc: 0.6888\n",
      "Epoch 54/100\n",
      "571/571 [==============================] - 0s 68us/step - loss: 0.5939 - acc: 0.6646 - val_loss: 0.5894 - val_acc: 0.6888\n",
      "Epoch 55/100\n",
      "571/571 [==============================] - 0s 72us/step - loss: 0.5930 - acc: 0.6655 - val_loss: 0.5884 - val_acc: 0.6888\n",
      "Epoch 56/100\n",
      "571/571 [==============================] - 0s 87us/step - loss: 0.5920 - acc: 0.6734 - val_loss: 0.5875 - val_acc: 0.6888\n",
      "Epoch 57/100\n",
      "571/571 [==============================] - 0s 79us/step - loss: 0.5911 - acc: 0.6743 - val_loss: 0.5866 - val_acc: 0.6958\n",
      "Epoch 58/100\n",
      "571/571 [==============================] - 0s 69us/step - loss: 0.5902 - acc: 0.6778 - val_loss: 0.5857 - val_acc: 0.6993\n",
      "Epoch 59/100\n",
      "571/571 [==============================] - 0s 83us/step - loss: 0.5893 - acc: 0.6769 - val_loss: 0.5848 - val_acc: 0.7028\n",
      "Epoch 60/100\n",
      "571/571 [==============================] - 0s 93us/step - loss: 0.5884 - acc: 0.6786 - val_loss: 0.5839 - val_acc: 0.7028\n",
      "Epoch 61/100\n",
      "571/571 [==============================] - 0s 78us/step - loss: 0.5874 - acc: 0.6830 - val_loss: 0.5830 - val_acc: 0.7098\n",
      "Epoch 62/100\n",
      "571/571 [==============================] - 0s 80us/step - loss: 0.5866 - acc: 0.6891 - val_loss: 0.5822 - val_acc: 0.7133\n",
      "Epoch 63/100\n",
      "571/571 [==============================] - 0s 69us/step - loss: 0.5857 - acc: 0.6891 - val_loss: 0.5814 - val_acc: 0.7098\n",
      "Epoch 64/100\n",
      "571/571 [==============================] - 0s 62us/step - loss: 0.5850 - acc: 0.6970 - val_loss: 0.5805 - val_acc: 0.7168\n",
      "Epoch 65/100\n",
      "571/571 [==============================] - 0s 67us/step - loss: 0.5841 - acc: 0.7049 - val_loss: 0.5797 - val_acc: 0.7168\n",
      "Epoch 66/100\n",
      "571/571 [==============================] - 0s 64us/step - loss: 0.5832 - acc: 0.7075 - val_loss: 0.5789 - val_acc: 0.7168\n",
      "Epoch 67/100\n",
      "571/571 [==============================] - 0s 82us/step - loss: 0.5825 - acc: 0.7119 - val_loss: 0.5781 - val_acc: 0.7168\n",
      "Epoch 68/100\n",
      "571/571 [==============================] - 0s 89us/step - loss: 0.5816 - acc: 0.7128 - val_loss: 0.5773 - val_acc: 0.7238\n",
      "Epoch 69/100\n",
      "571/571 [==============================] - 0s 85us/step - loss: 0.5808 - acc: 0.7154 - val_loss: 0.5765 - val_acc: 0.7308\n",
      "Epoch 70/100\n",
      "571/571 [==============================] - 0s 82us/step - loss: 0.5800 - acc: 0.7198 - val_loss: 0.5757 - val_acc: 0.7308\n",
      "Epoch 71/100\n",
      "571/571 [==============================] - 0s 65us/step - loss: 0.5793 - acc: 0.7215 - val_loss: 0.5750 - val_acc: 0.7308\n",
      "Epoch 72/100\n",
      "571/571 [==============================] - 0s 64us/step - loss: 0.5785 - acc: 0.7250 - val_loss: 0.5742 - val_acc: 0.7343\n",
      "Epoch 73/100\n",
      "571/571 [==============================] - 0s 72us/step - loss: 0.5778 - acc: 0.7285 - val_loss: 0.5735 - val_acc: 0.7308\n",
      "Epoch 74/100\n",
      "571/571 [==============================] - 0s 84us/step - loss: 0.5770 - acc: 0.7329 - val_loss: 0.5727 - val_acc: 0.7308\n",
      "Epoch 75/100\n",
      "571/571 [==============================] - 0s 70us/step - loss: 0.5762 - acc: 0.7347 - val_loss: 0.5720 - val_acc: 0.7308\n",
      "Epoch 76/100\n",
      "571/571 [==============================] - 0s 64us/step - loss: 0.5756 - acc: 0.7373 - val_loss: 0.5713 - val_acc: 0.7343\n",
      "Epoch 77/100\n",
      "571/571 [==============================] - 0s 82us/step - loss: 0.5749 - acc: 0.7434 - val_loss: 0.5705 - val_acc: 0.7413\n",
      "Epoch 78/100\n",
      "571/571 [==============================] - 0s 67us/step - loss: 0.5742 - acc: 0.7469 - val_loss: 0.5698 - val_acc: 0.7413\n",
      "Epoch 79/100\n",
      "571/571 [==============================] - 0s 67us/step - loss: 0.5735 - acc: 0.7461 - val_loss: 0.5691 - val_acc: 0.7483\n",
      "Epoch 80/100\n",
      "571/571 [==============================] - 0s 81us/step - loss: 0.5728 - acc: 0.7487 - val_loss: 0.5684 - val_acc: 0.7517\n",
      "Epoch 81/100\n",
      "571/571 [==============================] - 0s 67us/step - loss: 0.5721 - acc: 0.7522 - val_loss: 0.5677 - val_acc: 0.7587\n",
      "Epoch 82/100\n",
      "571/571 [==============================] - 0s 65us/step - loss: 0.5714 - acc: 0.7504 - val_loss: 0.5670 - val_acc: 0.7622\n",
      "Epoch 83/100\n",
      "571/571 [==============================] - 0s 75us/step - loss: 0.5707 - acc: 0.7566 - val_loss: 0.5664 - val_acc: 0.7657\n",
      "Epoch 84/100\n",
      "571/571 [==============================] - 0s 70us/step - loss: 0.5700 - acc: 0.7557 - val_loss: 0.5657 - val_acc: 0.7657\n",
      "Epoch 85/100\n",
      "571/571 [==============================] - 0s 75us/step - loss: 0.5694 - acc: 0.7566 - val_loss: 0.5651 - val_acc: 0.7727\n",
      "Epoch 86/100\n",
      "571/571 [==============================] - 0s 83us/step - loss: 0.5688 - acc: 0.7574 - val_loss: 0.5644 - val_acc: 0.7727\n",
      "Epoch 87/100\n",
      "571/571 [==============================] - 0s 79us/step - loss: 0.5681 - acc: 0.7609 - val_loss: 0.5638 - val_acc: 0.7692\n",
      "Epoch 88/100\n",
      "571/571 [==============================] - 0s 73us/step - loss: 0.5675 - acc: 0.7627 - val_loss: 0.5631 - val_acc: 0.7727\n",
      "Epoch 89/100\n",
      "571/571 [==============================] - 0s 75us/step - loss: 0.5669 - acc: 0.7636 - val_loss: 0.5625 - val_acc: 0.7762\n",
      "Epoch 90/100\n",
      "571/571 [==============================] - 0s 82us/step - loss: 0.5662 - acc: 0.7653 - val_loss: 0.5619 - val_acc: 0.7762\n",
      "Epoch 91/100\n",
      "571/571 [==============================] - 0s 70us/step - loss: 0.5656 - acc: 0.7644 - val_loss: 0.5613 - val_acc: 0.7692\n",
      "Epoch 92/100\n",
      "571/571 [==============================] - 0s 67us/step - loss: 0.5651 - acc: 0.7688 - val_loss: 0.5607 - val_acc: 0.7692\n",
      "Epoch 93/100\n",
      "571/571 [==============================] - 0s 69us/step - loss: 0.5645 - acc: 0.7706 - val_loss: 0.5601 - val_acc: 0.7692\n",
      "Epoch 94/100\n",
      "571/571 [==============================] - 0s 84us/step - loss: 0.5638 - acc: 0.7706 - val_loss: 0.5595 - val_acc: 0.7692\n",
      "Epoch 95/100\n",
      "571/571 [==============================] - 0s 78us/step - loss: 0.5633 - acc: 0.7741 - val_loss: 0.5588 - val_acc: 0.7692\n",
      "Epoch 96/100\n",
      "571/571 [==============================] - 0s 70us/step - loss: 0.5627 - acc: 0.7723 - val_loss: 0.5583 - val_acc: 0.7692\n",
      "Epoch 97/100\n",
      "571/571 [==============================] - 0s 96us/step - loss: 0.5621 - acc: 0.7741 - val_loss: 0.5577 - val_acc: 0.7692\n",
      "Epoch 98/100\n",
      "571/571 [==============================] - 0s 71us/step - loss: 0.5616 - acc: 0.7732 - val_loss: 0.5571 - val_acc: 0.7762\n",
      "Epoch 99/100\n",
      "571/571 [==============================] - 0s 70us/step - loss: 0.5611 - acc: 0.7750 - val_loss: 0.5565 - val_acc: 0.7797\n",
      "Epoch 100/100\n",
      "571/571 [==============================] - 0s 65us/step - loss: 0.5605 - acc: 0.7776 - val_loss: 0.5560 - val_acc: 0.7797\n"
     ]
    }
   ],
   "source": [
    "log_reg = Sequential()\n",
    "log_reg.add(Dense(2, input_dim=4, activation='sigmoid'))\n",
    "log_reg.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "hist = log_reg.fit(titanic_X_train, titanic_y_train, batch_size=20, epochs=100, verbose=1,\n",
    "            validation_data=(titanic_X_test, titanic_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test: 0.7797202776362012\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for test: {}'.format(hist.history['val_acc'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим еще скрытый слой и увеличим чило нейронов на входном слое. Так же во время обучения и валидации сохраним ту модель, которая покажет наилучшую accuracy на валидационном датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 571 samples, validate on 143 samples\n",
      "Epoch 1/20\n",
      "571/571 [==============================] - 1s 2ms/step - loss: 0.6500 - acc: 0.6095 - val_loss: 0.6290 - val_acc: 0.6049\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.60490, saving model to ./models/best_titanic_model.hdf5\n",
      "Epoch 2/20\n",
      "571/571 [==============================] - 0s 534us/step - loss: 0.6166 - acc: 0.6235 - val_loss: 0.5974 - val_acc: 0.6713\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.60490 to 0.67133, saving model to ./models/best_titanic_model.hdf5\n",
      "Epoch 3/20\n",
      "571/571 [==============================] - 0s 547us/step - loss: 0.5818 - acc: 0.7049 - val_loss: 0.5542 - val_acc: 0.7587\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.67133 to 0.75874, saving model to ./models/best_titanic_model.hdf5\n",
      "Epoch 4/20\n",
      "571/571 [==============================] - 0s 560us/step - loss: 0.5442 - acc: 0.7574 - val_loss: 0.5190 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.75874 to 0.78671, saving model to ./models/best_titanic_model.hdf5\n",
      "Epoch 5/20\n",
      "571/571 [==============================] - 0s 574us/step - loss: 0.5186 - acc: 0.7723 - val_loss: 0.4979 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.78671 to 0.80420, saving model to ./models/best_titanic_model.hdf5\n",
      "Epoch 6/20\n",
      "571/571 [==============================] - 0s 545us/step - loss: 0.5043 - acc: 0.7855 - val_loss: 0.4856 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.80420 to 0.81119, saving model to ./models/best_titanic_model.hdf5\n",
      "Epoch 7/20\n",
      "571/571 [==============================] - 0s 570us/step - loss: 0.4964 - acc: 0.7872 - val_loss: 0.4780 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.81119\n",
      "Epoch 8/20\n",
      "571/571 [==============================] - 0s 563us/step - loss: 0.4901 - acc: 0.7960 - val_loss: 0.4738 - val_acc: 0.8007\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81119\n",
      "Epoch 9/20\n",
      "571/571 [==============================] - 0s 559us/step - loss: 0.4856 - acc: 0.7881 - val_loss: 0.4692 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81119\n",
      "Epoch 10/20\n",
      "571/571 [==============================] - 0s 564us/step - loss: 0.4834 - acc: 0.7890 - val_loss: 0.4668 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81119\n",
      "Epoch 11/20\n",
      "571/571 [==============================] - 0s 570us/step - loss: 0.4804 - acc: 0.7942 - val_loss: 0.4638 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.81119\n",
      "Epoch 12/20\n",
      "571/571 [==============================] - 0s 565us/step - loss: 0.4765 - acc: 0.7837 - val_loss: 0.4669 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.81119\n",
      "Epoch 13/20\n",
      "571/571 [==============================] - 0s 586us/step - loss: 0.4748 - acc: 0.7907 - val_loss: 0.4594 - val_acc: 0.8007\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.81119\n",
      "Epoch 14/20\n",
      "571/571 [==============================] - 0s 553us/step - loss: 0.4730 - acc: 0.7820 - val_loss: 0.4638 - val_acc: 0.7657\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.81119\n",
      "Epoch 15/20\n",
      "571/571 [==============================] - 0s 555us/step - loss: 0.4714 - acc: 0.7846 - val_loss: 0.4597 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.81119\n",
      "Epoch 16/20\n",
      "571/571 [==============================] - 0s 572us/step - loss: 0.4690 - acc: 0.7863 - val_loss: 0.4553 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.81119\n",
      "Epoch 17/20\n",
      "571/571 [==============================] - 0s 556us/step - loss: 0.4663 - acc: 0.7855 - val_loss: 0.4542 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.81119\n",
      "Epoch 18/20\n",
      "571/571 [==============================] - 0s 573us/step - loss: 0.4653 - acc: 0.7863 - val_loss: 0.4543 - val_acc: 0.8007\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.81119\n",
      "Epoch 19/20\n",
      "571/571 [==============================] - 0s 571us/step - loss: 0.4635 - acc: 0.7916 - val_loss: 0.4521 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.81119\n",
      "Epoch 20/20\n",
      "571/571 [==============================] - 0s 579us/step - loss: 0.4635 - acc: 0.7916 - val_loss: 0.4527 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.81119\n"
     ]
    }
   ],
   "source": [
    "log_reg = Sequential()\n",
    "log_reg.add(Dense(8, input_dim=4, kernel_initializer='glorot_normal', activation='relu'))\n",
    "log_reg.add(Dense(20, kernel_initializer='glorot_normal', activation='relu'))\n",
    "log_reg.add(Dense(2, activation='sigmoid'))\n",
    "log_reg.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# checkpoint\n",
    "filepath='./models/best_titanic_model.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "hist = log_reg.fit(titanic_X_train, titanic_y_train, batch_size=3, epochs=20, verbose=1,\n",
    "                   validation_data=(titanic_X_test, titanic_y_test), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/best_titanic_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 77us/step\n",
      "Accuracy for test: 0.8111888124392583\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for test: {}'.format(model.evaluate(titanic_X_test, titanic_y_test)[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
